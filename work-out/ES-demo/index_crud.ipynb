{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n",
      "03 Aug 2020 DAQ FilesPQC887\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import re\n",
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from collections import deque\n",
    "from elasticsearch import Elasticsearch,helpers\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from elasticsearch.helpers import streaming_bulk\n",
    "from es_pandas import es_pandas\n",
    "from elasticsearch_dsl import Search\n",
    "\n",
    "import ujson as json\n",
    "#import commond.ipynb from same folder\n",
    "import import_ipynb\n",
    "from common import *\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n",
    ")\n",
    "handler.setFormatter(formatter)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_create_index_dsl(shrads=1):\n",
    "    request_body = {'settings': {'number_of_shards': shrads,\n",
    "                    'number_of_replicas': 2},\n",
    "                    'mappings': {'properties': {'timestamp': {'type': 'date',\n",
    "                    'format': 'yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis'}}}}\n",
    "\n",
    "    return request_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index(es, index_name):\n",
    "    if es.indices.exists(index_name):\n",
    "        res = es.indices.delete(index=index_name)\n",
    "        print(\"delete res: \", res)\n",
    "    else:\n",
    "        print(\"%s does not existing\" % index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es, index_name, shards):\n",
    "    if es.indices.exists(index_name):\n",
    "        print(f\"{index_name} exists\")\n",
    "\n",
    "    request_body = get_create_index_dsl(shards)\n",
    "    res = es.indices.create(index=index_name, body=request_body, timeout=\"60s\")\n",
    "    print(\"%s is created\" % index_name)\n",
    "    print('%s is done!' % sys._getframe().f_code.co_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingrest_bulk_json(json_file, index_name):\n",
    "    try:\n",
    "        # make the bulk call, and get a response\n",
    "        results = list(parallel_bulk(client=es, actions=json_file, index=index_name,chunk_size=1000, thread_count=4, queue_size=16))\n",
    "        elf.assertTrue(len(set([r[1] for r in results])) > 1)\n",
    "        print (\"\\nRESPONSE:\", results)\n",
    "    except Exception as e:\n",
    "        print(\"\\nERROR:\", e)\n",
    "    print('%s is done!' % sys._getframe().f_code.co_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host = \"http://elasticsearch-7.apaas-ppe1.eniot.io\"\n",
    "index_name = \"benchmark_base\"\n",
    "shards = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'new-index'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create index\n",
    "es = Elasticsearch(es_host, port=80)\n",
    "es.indices.create(index='new-index', request_timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete Index\n",
    "\n",
    "es = Elasticsearch(es_host, port=80)\n",
    "dict = es.indices.delete(index='new-index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete res:  {'acknowledged': True}\n",
      "raw_index_test is created\n",
      "create_index is done!\n"
     ]
    }
   ],
   "source": [
    "## use DSL to create index\n",
    "index_name = \"raw_index_test\"\n",
    "shards = 1\n",
    "\n",
    "delete_index(es, index_name)\n",
    "es = Elasticsearch(es_host, port=80)\n",
    "\n",
    "if create_index(es, index_name, shards):\n",
    "    print('<%s> is created successfuly'% index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR: ('11 document(s) failed to index.', [{'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'CWPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 's'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'CmPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'a'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'C2PMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'm'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'DGPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'p'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'DWPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'l'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'DmPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'e'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'D2PMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': '.'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'EGPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'j'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'EWPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 's'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'EmPMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'o'}}, {'index': {'_index': 'raw_index_test', '_type': '_doc', '_id': 'E2PMd3kBmGua5MiEF7p8', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'not_x_content_exception', 'reason': 'Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes'}}, 'data': 'n'}}])\n",
      "ingrest_bulk_json is done!\n"
     ]
    }
   ],
   "source": [
    "# Ingest bulk from json file, which is generated from filename.\n",
    "json_file = 'sample.json'\n",
    "\n",
    "ingrest_bulk_json(json_file, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**list all index**\n",
    "GET /_cat/indices  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
