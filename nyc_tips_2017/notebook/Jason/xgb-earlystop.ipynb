{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basics library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm.libsvm import cross_validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "'''use this model_selection rather than grid-server,\n",
    "#  https://stackovlem o ferflow.com/questions/40257492/gridsearchcv-typeerror-stratifiedkfold-object-is-not-iterable?rq=1\n",
    "#  can solve problem of “'StratifiedKFold' object is not iterable”\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#import commond.ipynb from same folder\n",
    "import import_ipynb\n",
    "from common_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
      "0         2  2017-01-07 12:47:25   2017-01-07 13:07:19                  N   \n",
      "1         2  2017-01-21 12:21:04   2017-01-21 12:22:57                  N   \n",
      "2         2  2017-01-04 08:53:29   2017-01-04 09:08:15                  N   \n",
      "3         2  2017-01-18 18:17:40   2017-01-18 18:24:12                  N   \n",
      "4         2  2017-01-09 14:56:30   2017-01-09 15:17:13                  N   \n",
      "\n",
      "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
      "0           1            40           141              1.0           2.91   \n",
      "1           1            74            74              1.0           0.36   \n",
      "2           1            65           143              2.0           2.31   \n",
      "3           1           165            40              1.0           0.96   \n",
      "4           1            94           120              1.0           2.86   \n",
      "\n",
      "   fare_amount  ...  ehail_fee  improvement_surcharge  total_amount  \\\n",
      "0         13.0  ...        NaN                    0.3         16.56   \n",
      "1          3.5  ...        NaN                    0.3          4.30   \n",
      "2         12.5  ...        NaN                    0.3         15.96   \n",
      "3          6.5  ...        NaN                    0.3          9.96   \n",
      "4         15.0  ...        NaN                    0.3         18.96   \n",
      "\n",
      "   payment_type  trip_type  duration  day_of_week  weekend      speed  tip  \n",
      "0             1        1.0    1194.0            5        1   8.773869    1  \n",
      "1             2        1.0     113.0            5        1  11.469027    0  \n",
      "2             1        1.0     886.0            2        0   9.386005    1  \n",
      "3             1        1.0     392.0            2        0   8.816327    1  \n",
      "4             1        1.0    1243.0            0        0   8.283186    1  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "   VendorID  RatecodeID  PULocationID  DOLocationID  passenger_count  \\\n",
      "0         2           1            40           141              1.0   \n",
      "1         2           1            74            74              1.0   \n",
      "2         2           1            65           143              2.0   \n",
      "3         2           1           165            40              1.0   \n",
      "4         2           1            94           120              1.0   \n",
      "\n",
      "   trip_distance  fare_amount  extra  tolls_amount  total_amount  \\\n",
      "0           2.91         13.0    0.0           0.0         16.56   \n",
      "1           0.36          3.5    0.0           0.0          4.30   \n",
      "2           2.31         12.5    0.0           0.0         15.96   \n",
      "3           0.96          6.5    1.0           0.0          9.96   \n",
      "4           2.86         15.0    0.0           0.0         18.96   \n",
      "\n",
      "   payment_type  duration  day_of_week      speed  tip_amount  \n",
      "0             1    1194.0            5   8.773869        2.76  \n",
      "1             2     113.0            5  11.469027        0.00  \n",
      "2             1     886.0            2   9.386005        2.66  \n",
      "3             1     392.0            2   8.816327        1.66  \n",
      "4             1    1243.0            0   8.283186        3.16  \n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = label_data_asArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.76 0.   2.66 ... 0.   7.96 0.  ]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09, 1.1, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.2, 1.21, 1.22, 1.23, 1.24, 1.25, 1.26, 1.27, 1.28, 1.29, 1.3, 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37, 1.38, 1.39, 1.4, 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48, 1.49, 1.5, 1.51, 1.52, 1.53, 1.54, 1.55, 1.56, 1.57, 1.58, 1.59, 1.6, 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.7, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 1.77, 1.78, 1.79, 1.8, 1.81, 1.82, 1.83, 1.84, 1.85, 1.86, 1.87, 1.88, 1.89, 1.9, 1.91, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98, 1.99, 2.01, 2.02, 2.03, 2.04, 2.05, 2.06, 2.07, 2.08, 2.09, 2.1, 2.11, 2.12, 2.13, 2.14, 2.15, 2.16, 2.17, 2.18, 2.19, 2.2, 2.21, 2.22, 2.23, 2.24, 2.25, 2.26, 2.27, 2.28, 2.29, 2.3, 2.31, 2.32, 2.33, 2.34, 2.35, 2.36, 2.37, 2.38, 2.39, 2.4, 2.42, 2.43, 2.44, 2.45, 2.46, 2.47, 2.48, 2.49, 2.5, 2.51, 2.52, 2.53, 2.54, 2.55, 2.56, 2.57, 2.58, 2.59, 2.6, 2.61, 2.62, 2.63, 2.64, 2.65, 2.66, 2.67, 2.68, 2.69, 2.7, 2.71, 2.72, 2.73, 2.74, 2.75, 2.76, 2.77, 2.78, 2.79, 2.8, 2.82, 2.83, 2.84, 2.85, 2.86, 2.87, 2.88, 2.89, 2.9, 2.91, 2.92, 2.93, 2.94, 2.95, 2.96, 2.97, 2.98, 2.99, 3.02, 3.03, 3.04, 3.05, 3.06, 3.07, 3.08, 3.09, 3.1, 3.11, 3.12, 3.13, 3.14, 3.15, 3.16, 3.17, 3.18, 3.19, 3.2, 3.21, 3.22, 3.23, 3.24, 3.25, 3.26, 3.27, 3.28, 3.29, 3.3, 3.32, 3.33, 3.34, 3.35, 3.36, 3.37, 3.38, 3.39, 3.4, 3.41, 3.42, 3.43, 3.45, 3.46, 3.47, 3.48, 3.49, 3.5, 3.51, 3.52, 3.53, 3.54, 3.55, 3.56, 3.57, 3.58, 3.6, 3.62, 3.63, 3.64, 3.65, 3.66, 3.67, 3.68, 3.69, 3.7, 3.71, 3.72, 3.73, 3.74, 3.75, 3.76, 3.77, 3.78, 3.79, 3.8, 3.81, 3.82, 3.83, 3.84, 3.85, 3.86, 3.87, 3.88, 3.89, 3.9, 3.92, 3.93, 3.94, 3.95, 3.96, 3.97, 3.98, 3.99, 4.01, 4.02, 4.03, 4.04, 4.05, 4.06, 4.07, 4.08, 4.09, 4.1, 4.12, 4.13, 4.14, 4.15, 4.16, 4.17, 4.18, 4.19, 4.2, 4.21, 4.22, 4.23, 4.24, 4.25, 4.26, 4.27, 4.28, 4.29, 4.3, 4.31, 4.32, 4.33, 4.34, 4.35, 4.36, 4.37, 4.38, 4.39, 4.4, 4.41, 4.42, 4.43, 4.44, 4.45, 4.46, 4.47, 4.48, 4.5, 4.52, 4.53, 4.54, 4.55, 4.56, 4.57, 4.58, 4.59, 4.6, 4.61, 4.62, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68, 4.69, 4.7, 4.71, 4.72, 4.73, 4.74, 4.75, 4.76, 4.77, 4.78, 4.79, 4.8, 4.81, 4.82, 4.83, 4.84, 4.85, 4.86, 4.87, 4.88, 4.89, 4.9, 4.91, 4.92, 4.93, 4.95, 4.96, 4.97, 4.98, 4.99, 5.03, 5.04, 5.05, 5.06, 5.07, 5.08, 5.09, 5.1, 5.13, 5.14, 5.15, 5.16, 5.17, 5.18, 5.19, 5.2, 5.21, 5.22, 5.23, 5.25, 5.26, 5.27, 5.28, 5.29, 5.3, 5.32, 5.33, 5.34, 5.35, 5.36, 5.37, 5.38, 5.4, 5.42, 5.43, 5.44, 5.45, 5.46, 5.47, 5.48, 5.49, 5.5, 5.51, 5.52, 5.53, 5.54, 5.55, 5.56, 5.57, 5.58, 5.6, 5.63, 5.64, 5.65, 5.66, 5.67, 5.68, 5.7, 5.71, 5.72, 5.73, 5.74, 5.75, 5.76, 5.77, 5.78, 5.79, 5.8, 5.81, 5.82, 5.83, 5.84, 5.85, 5.86, 5.87, 5.88, 5.9, 5.91, 5.92, 5.93, 5.94, 5.95, 5.96, 5.97, 5.98, 5.99, 6.01, 6.04, 6.05, 6.06, 6.07, 6.08, 6.09, 6.1, 6.12, 6.13, 6.14, 6.15, 6.16, 6.17, 6.18, 6.19, 6.2, 6.21, 6.24, 6.25, 6.26, 6.27, 6.28, 6.3, 6.32, 6.33, 6.34, 6.35, 6.36, 6.37, 6.39, 6.4, 6.42, 6.43, 6.45, 6.46, 6.47, 6.49, 6.5, 6.51, 6.53, 6.54, 6.55, 6.56, 6.57, 6.58, 6.6, 6.61, 6.62, 6.63, 6.64, 6.65, 6.66, 6.67, 6.68, 6.69, 6.7, 6.71, 6.72, 6.73, 6.74, 6.75, 6.76, 6.77, 6.78, 6.79, 6.8, 6.81, 6.82, 6.83, 6.84, 6.85, 6.86, 6.87, 6.88, 6.89, 6.9, 6.92, 6.94, 6.95, 6.96, 6.97, 6.98, 6.99, 7.02, 7.05, 7.06, 7.07, 7.08, 7.09, 7.1, 7.11, 7.14, 7.15, 7.16, 7.17, 7.18, 7.2, 7.21, 7.24, 7.25, 7.26, 7.27, 7.28, 7.29, 7.3, 7.32, 7.33, 7.34, 7.35, 7.36, 7.37, 7.4, 7.41, 7.43, 7.44, 7.45, 7.46, 7.47, 7.48, 7.5, 7.52, 7.53, 7.54, 7.55, 7.56, 7.57, 7.58, 7.59, 7.6, 7.61, 7.62, 7.63, 7.64, 7.65, 7.66, 7.67, 7.68, 7.7, 7.71, 7.74, 7.75, 7.76, 7.77, 7.78, 7.8, 7.82, 7.84, 7.85, 7.86, 7.87, 7.88, 7.89, 7.9, 7.92, 7.93, 7.95, 7.96, 7.97, 7.99, 8.02, 8.03, 8.04, 8.05, 8.06, 8.07, 8.08, 8.1, 8.11, 8.13, 8.14, 8.15, 8.16, 8.17, 8.19, 8.2, 8.21, 8.22, 8.24, 8.25, 8.26, 8.27, 8.28, 8.3, 8.32, 8.33, 8.34, 8.35, 8.36, 8.37, 8.38, 8.4, 8.44, 8.45, 8.46, 8.47, 8.49, 8.5, 8.52, 8.53, 8.54, 8.55, 8.56, 8.57, 8.58, 8.6, 8.64, 8.65, 8.66, 8.67, 8.68, 8.69, 8.7, 8.71, 8.72, 8.74, 8.75, 8.76, 8.77, 8.79, 8.8, 8.82, 8.83, 8.84, 8.85, 8.86, 8.87, 8.88, 8.9, 8.91, 8.94, 8.95, 8.96, 8.97, 8.98, 9.01, 9.03, 9.04, 9.05, 9.06, 9.07, 9.08, 9.09, 9.1, 9.12, 9.13, 9.15, 9.16, 9.17, 9.2, 9.21, 9.24, 9.25, 9.26, 9.27, 9.28, 9.3, 9.32, 9.34, 9.35, 9.36, 9.37, 9.38, 9.39, 9.4, 9.42, 9.43, 9.44, 9.45, 9.46, 9.47, 9.48, 9.5, 9.54, 9.55, 9.56, 9.57, 9.58, 9.6, 9.64, 9.65, 9.66, 9.67, 9.68, 9.69, 9.7, 9.71, 9.72, 9.75, 9.76, 9.77, 9.8, 9.82, 9.83, 9.84, 9.85, 9.86, 9.87, 9.88, 9.9, 9.94, 9.95, 9.96, 9.97, 9.99, 10.03, 10.05, 10.06, 10.07, 10.08, 10.09, 10.1, 10.11, 10.14, 10.15, 10.16, 10.17, 10.2, 10.21, 10.23, 10.24, 10.25, 10.26, 10.27, 10.29, 10.3, 10.32, 10.34, 10.35, 10.36, 10.37, 10.38, 10.4, 10.44, 10.45, 10.46, 10.47, 10.5, 10.54, 10.55, 10.56, 10.57, 10.58, 10.59, 10.6, 10.62, 10.63, 10.65, 10.66, 10.67, 10.68, 10.69, 10.7, 10.71, 10.74, 10.75, 10.76, 10.77, 10.78, 10.8, 10.82, 10.84, 10.85, 10.86, 10.87, 10.88, 10.89, 10.9, 10.92, 10.95, 10.96, 10.97, 11.04, 11.05, 11.06, 11.07, 11.08, 11.1, 11.11, 11.14, 11.15, 11.16, 11.17, 11.18, 11.19, 11.2, 11.21, 11.25, 11.26, 11.27, 11.3, 11.32, 11.34, 11.35, 11.36, 11.37, 11.38, 11.4, 11.44, 11.45, 11.46, 11.47, 11.48, 11.49, 11.5, 11.51, 11.55, 11.56, 11.57, 11.58, 11.6, 11.64, 11.65, 11.66, 11.67, 11.68, 11.7, 11.71, 11.72, 11.75, 11.76, 11.77, 11.79, 11.8, 11.82, 11.84, 11.85, 11.86, 11.87, 11.88, 11.9, 11.91, 11.94, 11.95, 11.96, 11.97, 11.98, 12.01, 12.05, 12.06, 12.07, 12.08, 12.09, 12.1, 12.11, 12.12, 12.14, 12.15, 12.16, 12.17, 12.18, 12.2, 12.21, 12.22, 12.24, 12.25, 12.26, 12.27, 12.28, 12.3, 12.32, 12.33, 12.34, 12.35, 12.36, 12.37, 12.39, 12.4, 12.45, 12.46, 12.47, 12.5, 12.52, 12.54, 12.55, 12.56, 12.57, 12.58, 12.6, 12.65, 12.66, 12.67, 12.69, 12.7, 12.71, 12.72, 12.76, 12.77, 12.8, 12.82, 12.84, 12.85, 12.86, 12.87, 12.88, 12.92, 12.95, 12.96, 12.97, 12.99, 13.05, 13.06, 13.07, 13.08, 13.1, 13.11, 13.13, 13.14, 13.15, 13.16, 13.17, 13.2, 13.25, 13.26, 13.27, 13.29, 13.3, 13.32, 13.34, 13.36, 13.37, 13.44, 13.45, 13.46, 13.47, 13.5, 13.55, 13.56, 13.57, 13.58, 13.59, 13.6, 13.65, 13.66, 13.67, 13.7, 13.71, 13.74, 13.75, 13.76, 13.8, 13.82, 13.84, 13.85, 13.86, 13.87, 13.89, 13.9, 13.93, 13.95, 13.96, 13.97, 14.04, 14.05, 14.06, 14.08, 14.1, 14.11, 14.15, 14.16, 14.17, 14.18, 14.19, 14.2, 14.21, 14.22, 14.25, 14.26, 14.3, 14.32, 14.34, 14.35, 14.36, 14.38, 14.4, 14.45, 14.46, 14.49, 14.5, 14.51, 14.53, 14.55, 14.56, 14.57, 14.58, 14.59, 14.6, 14.62, 14.64, 14.65, 14.66, 14.7, 14.71, 14.75, 14.76, 14.79, 14.8, 14.82, 14.84, 14.85, 14.86, 14.87, 14.88, 14.9, 14.94, 14.95, 14.96, 14.97, 15.05, 15.06, 15.08, 15.09, 15.1, 15.11, 15.12, 15.14, 15.15, 15.16, 15.17, 15.18, 15.2, 15.24, 15.25, 15.26, 15.27, 15.28, 15.3, 15.32, 15.34, 15.36, 15.39, 15.4, 15.45, 15.46, 15.5, 15.54, 15.55, 15.56, 15.58, 15.6, 15.66, 15.69, 15.7, 15.71, 15.75, 15.76, 15.78, 15.8, 15.82, 15.84, 15.85, 15.86, 15.88, 15.9, 15.95, 15.96, 15.98, 15.99, 16.05, 16.08, 16.15, 16.16, 16.18, 16.2, 16.25, 16.26, 16.27, 16.28, 16.29, 16.3, 16.32, 16.37, 16.38, 16.4, 16.44, 16.45, 16.46, 16.5, 16.55, 16.56, 16.58, 16.59, 16.6, 16.66, 16.68, 16.7, 16.74, 16.76, 16.84, 16.85, 16.86, 16.89, 16.9, 16.95, 16.96, 17.05, 17.06, 17.07, 17.1, 17.11, 17.12, 17.15, 17.16, 17.17, 17.19, 17.2, 17.26, 17.3, 17.32, 17.34, 17.4, 17.45, 17.46, 17.49, 17.5, 17.55, 17.56, 17.58, 17.64, 17.65, 17.7, 17.75, 17.76, 17.77, 17.79, 17.8, 17.86, 17.87, 17.94, 17.95, 17.96, 18.05, 18.06, 18.09, 18.15, 18.16, 18.2, 18.24, 18.26, 18.27, 18.3, 18.32, 18.37, 18.45, 18.48, 18.5, 18.54, 18.56, 18.57, 18.58, 18.62, 18.65, 18.66, 18.69, 18.75, 18.76, 18.84, 18.85, 18.86, 18.95, 18.96, 18.99, 19.05, 19.06, 19.1, 19.15, 19.16, 19.2, 19.25, 19.26, 19.29, 19.3, 19.32, 19.34, 19.36, 19.4, 19.44, 19.45, 19.5, 19.55, 19.56, 19.58, 19.59, 19.6, 19.7, 19.74, 19.75, 19.76, 19.77, 19.78, 19.8, 19.82, 19.85, 19.86, 19.91, 19.96, 19.99, 20.02, 20.04, 20.06, 20.07, 20.08, 20.1, 20.19, 20.2, 20.21, 20.25, 20.26, 20.3, 20.34, 20.35, 20.36, 20.37, 20.38, 20.4, 20.46, 20.49, 20.5, 20.58, 20.61, 20.64, 20.65, 20.67, 20.7, 20.71, 20.79, 20.8, 20.86, 20.91, 20.94, 20.95, 20.96, 21.09, 21.12, 21.16, 21.18, 21.2, 21.24, 21.25, 21.27, 21.28, 21.3, 21.54, 21.58, 21.6, 21.66, 21.67, 21.69, 21.7, 21.8, 21.9, 21.95, 22.05, 22.08, 22.1, 22.14, 22.2, 22.22, 22.29, 22.34, 22.44, 22.45, 22.49, 22.5, 22.62, 22.7, 22.74, 22.76, 22.77, 22.8, 22.85, 22.95, 22.96, 23.15, 23.2, 23.23, 23.3, 23.45, 23.56, 23.66, 23.9, 23.92, 23.94, 23.96, 24.02, 24.06, 24.28, 24.35, 24.39, 24.42, 24.67, 24.82, 24.9, 24.96, 25.05, 25.15, 25.16, 25.2, 25.32, 25.44, 25.46, 25.67, 25.72, 25.8, 25.88, 25.89, 25.96, 26.3, 26.47, 26.5, 26.58, 26.65, 26.72, 26.92, 26.97, 27.09, 27.24, 27.3, 27.4, 27.44, 27.7, 27.8, 27.84, 28.21, 28.28, 28.4, 28.45, 28.5, 28.7, 28.72, 28.8, 28.98, 29.05, 29.15, 29.17, 29.2, 29.32, 29.36, 29.4, 29.5, 29.7, 29.75, 29.9, 30.2, 30.24, 30.7, 30.95, 30.96, 31.45, 31.54, 31.86, 32.17, 32.9, 33.33, 33.7, 34.67, 34.74, 35.21, 35.7, 36.3, 37.16, 37.17, 37.36, 37.78, 38.26, 38.51, 39.25, 40.25, 40.66, 42.4, 42.45, 42.7, 42.99, 43.15, 44.2, 44.44, 45.28, 45.45, 45.7, 46.2, 46.26, 46.7, 47.32, 47.7, 48.2, 54.97, 55.5, 55.89, 59.04, 60.08, 60.8, 61.57, 62.5, 69.08, 69.82, 77.7, 80.8, 85.45, 104.4, 105.52, 111.1, 125.88, 232.46]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0c22bff153ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoded_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logloss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# make predictions for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    676\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight_eval_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                         nthread=self.n_jobs)\n\u001b[0;32m--> 678\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m             )\n\u001b[1;32m    680\u001b[0m             \u001b[0mnevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    676\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight_eval_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                         nthread=self.n_jobs)\n\u001b[0;32m--> 678\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m             )\n\u001b[1;32m    680\u001b[0m             \u001b[0mnevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, encode)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[0;34m(values, uniques, encode)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             raise ValueError(\"y contains previously unseen labels: %s\"\n\u001b[0;32m---> 53\u001b[0;31m                              % str(diff))\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09, 1.1, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.2, 1.21, 1.22, 1.23, 1.24, 1.25, 1.26, 1.27, 1.28, 1.29, 1.3, 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37, 1.38, 1.39, 1.4, 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48, 1.49, 1.5, 1.51, 1.52, 1.53, 1.54, 1.55, 1.56, 1.57, 1.58, 1.59, 1.6, 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.7, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 1.77, 1.78, 1.79, 1.8, 1.81, 1.82, 1.83, 1.84, 1.85, 1.86, 1.87, 1.88, 1.89, 1.9, 1.91, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98, 1.99, 2.01, 2.02, 2.03, 2.04, 2.05, 2.06, 2.07, 2.08, 2.09, 2.1, 2.11, 2.12, 2.13, 2.14, 2.15, 2.16, 2.17, 2.18, 2.19, 2.2, 2.21, 2.22, 2.23, 2.24, 2.25, 2.26, 2.27, 2.28, 2.29, 2.3, 2.31, 2.32, 2.33, 2.34, 2.35, 2.36, 2.37, 2.38, 2.39, 2.4, 2.42, 2.43, 2.44, 2.45, 2.46, 2.47, 2.48, 2.49, 2.5, 2.51, 2.52, 2.53, 2.54, 2.55, 2.56, 2.57, 2.58, 2.59, 2.6, 2.61, 2.62, 2.63, 2.64, 2.65, 2.66, 2.67, 2.68, 2.69, 2.7, 2.71, 2.72, 2.73, 2.74, 2.75, 2.76, 2.77, 2.78, 2.79, 2.8, 2.82, 2.83, 2.84, 2.85, 2.86, 2.87, 2.88, 2.89, 2.9, 2.91, 2.92, 2.93, 2.94, 2.95, 2.96, 2.97, 2.98, 2.99, 3.02, 3.03, 3.04, 3.05, 3.06, 3.07, 3.08, 3.09, 3.1, 3.11, 3.12, 3.13, 3.14, 3.15, 3.16, 3.17, 3.18, 3.19, 3.2, 3.21, 3.22, 3.23, 3.24, 3.25, 3.26, 3.27, 3.28, 3.29, 3.3, 3.32, 3.33, 3.34, 3.35, 3.36, 3.37, 3.38, 3.39, 3.4, 3.41, 3.42, 3.43, 3.45, 3.46, 3.47, 3.48, 3.49, 3.5, 3.51, 3.52, 3.53, 3.54, 3.55, 3.56, 3.57, 3.58, 3.6, 3.62, 3.63, 3.64, 3.65, 3.66, 3.67, 3.68, 3.69, 3.7, 3.71, 3.72, 3.73, 3.74, 3.75, 3.76, 3.77, 3.78, 3.79, 3.8, 3.81, 3.82, 3.83, 3.84, 3.85, 3.86, 3.87, 3.88, 3.89, 3.9, 3.92, 3.93, 3.94, 3.95, 3.96, 3.97, 3.98, 3.99, 4.01, 4.02, 4.03, 4.04, 4.05, 4.06, 4.07, 4.08, 4.09, 4.1, 4.12, 4.13, 4.14, 4.15, 4.16, 4.17, 4.18, 4.19, 4.2, 4.21, 4.22, 4.23, 4.24, 4.25, 4.26, 4.27, 4.28, 4.29, 4.3, 4.31, 4.32, 4.33, 4.34, 4.35, 4.36, 4.37, 4.38, 4.39, 4.4, 4.41, 4.42, 4.43, 4.44, 4.45, 4.46, 4.47, 4.48, 4.5, 4.52, 4.53, 4.54, 4.55, 4.56, 4.57, 4.58, 4.59, 4.6, 4.61, 4.62, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68, 4.69, 4.7, 4.71, 4.72, 4.73, 4.74, 4.75, 4.76, 4.77, 4.78, 4.79, 4.8, 4.81, 4.82, 4.83, 4.84, 4.85, 4.86, 4.87, 4.88, 4.89, 4.9, 4.91, 4.92, 4.93, 4.95, 4.96, 4.97, 4.98, 4.99, 5.03, 5.04, 5.05, 5.06, 5.07, 5.08, 5.09, 5.1, 5.13, 5.14, 5.15, 5.16, 5.17, 5.18, 5.19, 5.2, 5.21, 5.22, 5.23, 5.25, 5.26, 5.27, 5.28, 5.29, 5.3, 5.32, 5.33, 5.34, 5.35, 5.36, 5.37, 5.38, 5.4, 5.42, 5.43, 5.44, 5.45, 5.46, 5.47, 5.48, 5.49, 5.5, 5.51, 5.52, 5.53, 5.54, 5.55, 5.56, 5.57, 5.58, 5.6, 5.63, 5.64, 5.65, 5.66, 5.67, 5.68, 5.7, 5.71, 5.72, 5.73, 5.74, 5.75, 5.76, 5.77, 5.78, 5.79, 5.8, 5.81, 5.82, 5.83, 5.84, 5.85, 5.86, 5.87, 5.88, 5.9, 5.91, 5.92, 5.93, 5.94, 5.95, 5.96, 5.97, 5.98, 5.99, 6.01, 6.04, 6.05, 6.06, 6.07, 6.08, 6.09, 6.1, 6.12, 6.13, 6.14, 6.15, 6.16, 6.17, 6.18, 6.19, 6.2, 6.21, 6.24, 6.25, 6.26, 6.27, 6.28, 6.3, 6.32, 6.33, 6.34, 6.35, 6.36, 6.37, 6.39, 6.4, 6.42, 6.43, 6.45, 6.46, 6.47, 6.49, 6.5, 6.51, 6.53, 6.54, 6.55, 6.56, 6.57, 6.58, 6.6, 6.61, 6.62, 6.63, 6.64, 6.65, 6.66, 6.67, 6.68, 6.69, 6.7, 6.71, 6.72, 6.73, 6.74, 6.75, 6.76, 6.77, 6.78, 6.79, 6.8, 6.81, 6.82, 6.83, 6.84, 6.85, 6.86, 6.87, 6.88, 6.89, 6.9, 6.92, 6.94, 6.95, 6.96, 6.97, 6.98, 6.99, 7.02, 7.05, 7.06, 7.07, 7.08, 7.09, 7.1, 7.11, 7.14, 7.15, 7.16, 7.17, 7.18, 7.2, 7.21, 7.24, 7.25, 7.26, 7.27, 7.28, 7.29, 7.3, 7.32, 7.33, 7.34, 7.35, 7.36, 7.37, 7.4, 7.41, 7.43, 7.44, 7.45, 7.46, 7.47, 7.48, 7.5, 7.52, 7.53, 7.54, 7.55, 7.56, 7.57, 7.58, 7.59, 7.6, 7.61, 7.62, 7.63, 7.64, 7.65, 7.66, 7.67, 7.68, 7.7, 7.71, 7.74, 7.75, 7.76, 7.77, 7.78, 7.8, 7.82, 7.84, 7.85, 7.86, 7.87, 7.88, 7.89, 7.9, 7.92, 7.93, 7.95, 7.96, 7.97, 7.99, 8.02, 8.03, 8.04, 8.05, 8.06, 8.07, 8.08, 8.1, 8.11, 8.13, 8.14, 8.15, 8.16, 8.17, 8.19, 8.2, 8.21, 8.22, 8.24, 8.25, 8.26, 8.27, 8.28, 8.3, 8.32, 8.33, 8.34, 8.35, 8.36, 8.37, 8.38, 8.4, 8.44, 8.45, 8.46, 8.47, 8.49, 8.5, 8.52, 8.53, 8.54, 8.55, 8.56, 8.57, 8.58, 8.6, 8.64, 8.65, 8.66, 8.67, 8.68, 8.69, 8.7, 8.71, 8.72, 8.74, 8.75, 8.76, 8.77, 8.79, 8.8, 8.82, 8.83, 8.84, 8.85, 8.86, 8.87, 8.88, 8.9, 8.91, 8.94, 8.95, 8.96, 8.97, 8.98, 9.01, 9.03, 9.04, 9.05, 9.06, 9.07, 9.08, 9.09, 9.1, 9.12, 9.13, 9.15, 9.16, 9.17, 9.2, 9.21, 9.24, 9.25, 9.26, 9.27, 9.28, 9.3, 9.32, 9.34, 9.35, 9.36, 9.37, 9.38, 9.39, 9.4, 9.42, 9.43, 9.44, 9.45, 9.46, 9.47, 9.48, 9.5, 9.54, 9.55, 9.56, 9.57, 9.58, 9.6, 9.64, 9.65, 9.66, 9.67, 9.68, 9.69, 9.7, 9.71, 9.72, 9.75, 9.76, 9.77, 9.8, 9.82, 9.83, 9.84, 9.85, 9.86, 9.87, 9.88, 9.9, 9.94, 9.95, 9.96, 9.97, 9.99, 10.03, 10.05, 10.06, 10.07, 10.08, 10.09, 10.1, 10.11, 10.14, 10.15, 10.16, 10.17, 10.2, 10.21, 10.23, 10.24, 10.25, 10.26, 10.27, 10.29, 10.3, 10.32, 10.34, 10.35, 10.36, 10.37, 10.38, 10.4, 10.44, 10.45, 10.46, 10.47, 10.5, 10.54, 10.55, 10.56, 10.57, 10.58, 10.59, 10.6, 10.62, 10.63, 10.65, 10.66, 10.67, 10.68, 10.69, 10.7, 10.71, 10.74, 10.75, 10.76, 10.77, 10.78, 10.8, 10.82, 10.84, 10.85, 10.86, 10.87, 10.88, 10.89, 10.9, 10.92, 10.95, 10.96, 10.97, 11.04, 11.05, 11.06, 11.07, 11.08, 11.1, 11.11, 11.14, 11.15, 11.16, 11.17, 11.18, 11.19, 11.2, 11.21, 11.25, 11.26, 11.27, 11.3, 11.32, 11.34, 11.35, 11.36, 11.37, 11.38, 11.4, 11.44, 11.45, 11.46, 11.47, 11.48, 11.49, 11.5, 11.51, 11.55, 11.56, 11.57, 11.58, 11.6, 11.64, 11.65, 11.66, 11.67, 11.68, 11.7, 11.71, 11.72, 11.75, 11.76, 11.77, 11.79, 11.8, 11.82, 11.84, 11.85, 11.86, 11.87, 11.88, 11.9, 11.91, 11.94, 11.95, 11.96, 11.97, 11.98, 12.01, 12.05, 12.06, 12.07, 12.08, 12.09, 12.1, 12.11, 12.12, 12.14, 12.15, 12.16, 12.17, 12.18, 12.2, 12.21, 12.22, 12.24, 12.25, 12.26, 12.27, 12.28, 12.3, 12.32, 12.33, 12.34, 12.35, 12.36, 12.37, 12.39, 12.4, 12.45, 12.46, 12.47, 12.5, 12.52, 12.54, 12.55, 12.56, 12.57, 12.58, 12.6, 12.65, 12.66, 12.67, 12.69, 12.7, 12.71, 12.72, 12.76, 12.77, 12.8, 12.82, 12.84, 12.85, 12.86, 12.87, 12.88, 12.92, 12.95, 12.96, 12.97, 12.99, 13.05, 13.06, 13.07, 13.08, 13.1, 13.11, 13.13, 13.14, 13.15, 13.16, 13.17, 13.2, 13.25, 13.26, 13.27, 13.29, 13.3, 13.32, 13.34, 13.36, 13.37, 13.44, 13.45, 13.46, 13.47, 13.5, 13.55, 13.56, 13.57, 13.58, 13.59, 13.6, 13.65, 13.66, 13.67, 13.7, 13.71, 13.74, 13.75, 13.76, 13.8, 13.82, 13.84, 13.85, 13.86, 13.87, 13.89, 13.9, 13.93, 13.95, 13.96, 13.97, 14.04, 14.05, 14.06, 14.08, 14.1, 14.11, 14.15, 14.16, 14.17, 14.18, 14.19, 14.2, 14.21, 14.22, 14.25, 14.26, 14.3, 14.32, 14.34, 14.35, 14.36, 14.38, 14.4, 14.45, 14.46, 14.49, 14.5, 14.51, 14.53, 14.55, 14.56, 14.57, 14.58, 14.59, 14.6, 14.62, 14.64, 14.65, 14.66, 14.7, 14.71, 14.75, 14.76, 14.79, 14.8, 14.82, 14.84, 14.85, 14.86, 14.87, 14.88, 14.9, 14.94, 14.95, 14.96, 14.97, 15.05, 15.06, 15.08, 15.09, 15.1, 15.11, 15.12, 15.14, 15.15, 15.16, 15.17, 15.18, 15.2, 15.24, 15.25, 15.26, 15.27, 15.28, 15.3, 15.32, 15.34, 15.36, 15.39, 15.4, 15.45, 15.46, 15.5, 15.54, 15.55, 15.56, 15.58, 15.6, 15.66, 15.69, 15.7, 15.71, 15.75, 15.76, 15.78, 15.8, 15.82, 15.84, 15.85, 15.86, 15.88, 15.9, 15.95, 15.96, 15.98, 15.99, 16.05, 16.08, 16.15, 16.16, 16.18, 16.2, 16.25, 16.26, 16.27, 16.28, 16.29, 16.3, 16.32, 16.37, 16.38, 16.4, 16.44, 16.45, 16.46, 16.5, 16.55, 16.56, 16.58, 16.59, 16.6, 16.66, 16.68, 16.7, 16.74, 16.76, 16.84, 16.85, 16.86, 16.89, 16.9, 16.95, 16.96, 17.05, 17.06, 17.07, 17.1, 17.11, 17.12, 17.15, 17.16, 17.17, 17.19, 17.2, 17.26, 17.3, 17.32, 17.34, 17.4, 17.45, 17.46, 17.49, 17.5, 17.55, 17.56, 17.58, 17.64, 17.65, 17.7, 17.75, 17.76, 17.77, 17.79, 17.8, 17.86, 17.87, 17.94, 17.95, 17.96, 18.05, 18.06, 18.09, 18.15, 18.16, 18.2, 18.24, 18.26, 18.27, 18.3, 18.32, 18.37, 18.45, 18.48, 18.5, 18.54, 18.56, 18.57, 18.58, 18.62, 18.65, 18.66, 18.69, 18.75, 18.76, 18.84, 18.85, 18.86, 18.95, 18.96, 18.99, 19.05, 19.06, 19.1, 19.15, 19.16, 19.2, 19.25, 19.26, 19.29, 19.3, 19.32, 19.34, 19.36, 19.4, 19.44, 19.45, 19.5, 19.55, 19.56, 19.58, 19.59, 19.6, 19.7, 19.74, 19.75, 19.76, 19.77, 19.78, 19.8, 19.82, 19.85, 19.86, 19.91, 19.96, 19.99, 20.02, 20.04, 20.06, 20.07, 20.08, 20.1, 20.19, 20.2, 20.21, 20.25, 20.26, 20.3, 20.34, 20.35, 20.36, 20.37, 20.38, 20.4, 20.46, 20.49, 20.5, 20.58, 20.61, 20.64, 20.65, 20.67, 20.7, 20.71, 20.79, 20.8, 20.86, 20.91, 20.94, 20.95, 20.96, 21.09, 21.12, 21.16, 21.18, 21.2, 21.24, 21.25, 21.27, 21.28, 21.3, 21.54, 21.58, 21.6, 21.66, 21.67, 21.69, 21.7, 21.8, 21.9, 21.95, 22.05, 22.08, 22.1, 22.14, 22.2, 22.22, 22.29, 22.34, 22.44, 22.45, 22.49, 22.5, 22.62, 22.7, 22.74, 22.76, 22.77, 22.8, 22.85, 22.95, 22.96, 23.15, 23.2, 23.23, 23.3, 23.45, 23.56, 23.66, 23.9, 23.92, 23.94, 23.96, 24.02, 24.06, 24.28, 24.35, 24.39, 24.42, 24.67, 24.82, 24.9, 24.96, 25.05, 25.15, 25.16, 25.2, 25.32, 25.44, 25.46, 25.67, 25.72, 25.8, 25.88, 25.89, 25.96, 26.3, 26.47, 26.5, 26.58, 26.65, 26.72, 26.92, 26.97, 27.09, 27.24, 27.3, 27.4, 27.44, 27.7, 27.8, 27.84, 28.21, 28.28, 28.4, 28.45, 28.5, 28.7, 28.72, 28.8, 28.98, 29.05, 29.15, 29.17, 29.2, 29.32, 29.36, 29.4, 29.5, 29.7, 29.75, 29.9, 30.2, 30.24, 30.7, 30.95, 30.96, 31.45, 31.54, 31.86, 32.17, 32.9, 33.33, 33.7, 34.67, 34.74, 35.21, 35.7, 36.3, 37.16, 37.17, 37.36, 37.78, 38.26, 38.51, 39.25, 40.25, 40.66, 42.4, 42.45, 42.7, 42.99, 43.15, 44.2, 44.44, 45.28, 45.45, 45.7, 46.2, 46.26, 46.7, 47.32, 47.7, 48.2, 54.97, 55.5, 55.89, 59.04, 60.08, 60.8, 61.57, 62.5, 69.08, 69.82, 77.7, 80.8, 85.45, 104.4, 105.52, 111.1, 125.88, 232.46]"
     ]
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "eval_set = [(X_test, y_test)]\n",
    "model.fit(X_train, label_encoded_y, early_stopping_rounds=2, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
